{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkYT6K7Bp2T5",
        "outputId": "c340e9a8-30d5-4859-f4f3-6e31d7e255d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrIjg86XqYUs",
        "outputId": "c2e57de9-b927-4e93-c52e-84a94a705e8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf .git\n",
        "# !git clone https://github.com/bst-depractice/spark_play"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwltLOPWcM48",
        "outputId": "468ced92-33a9-422f-f16d-c6b88d3d4ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spark_play'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 15 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (15/15), 1.17 MiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs"
      },
      "source": [
        "!apt-get -qq update > /tmp/apt.out\n",
        "!apt-get install -y -qq openjdk-11-jdk-headless"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(wget -q --show-progress -nc https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "32LLPBQpij-w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-e1DAsGUaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55851fd7-3568-4e7b-9d22-1927b28a1af9"
      },
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta, pyngrok\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark==3.2.1\n",
        "  %pip install -q findspark\n",
        "  %pip install -q delta\n",
        "  %pip install pyngrok"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the config k,v pairs and get a spark session object"
      ],
      "metadata": {
        "id": "7CBeyfwbK61a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/drive/MyDrive/Github/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "findspark.init()\n",
        "MAX_MEMORY=\"8g\"\n",
        "maven_coords = [\n",
        "    \"org.apache.spark:spark-avro_2.12:3.2.1\",\n",
        "    \"io.delta:delta-core_2.12:2.0.0rc1\",\n",
        "    \"org.xerial:sqlite-jdbc:3.36.0.3\",\n",
        "    \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\",\n",
        "    \"com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.8\",\n",
        "]\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(maven_coords))\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY)\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
        "    .config('spark.ui.port', '4050')\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        "    )\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "nyXkqLzrjJ1c",
        "outputId": "9ad9736b-4039-4d0a-edcd-96d3cb703876"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bed701835e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9d70b7f7a91c:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied \"\n",
        "\"from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4040\n",
        "public_url = ngrok.connect(ui_port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{ui_port}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dqyNpYQTDIM",
        "outputId": "02e254f3-6ac5-4613-b5cf-1956318ec998"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-15T14:39:21+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://50f0-34-68-94-133.ngrok-free.app\" -> \"http://127.0.0.1:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup is complete. At this point you have started a spark application and able to access the application-ui using the url above.\n",
        "You can start writing your data transformation code below..."
      ],
      "metadata": {
        "id": "PFIP7ijrWE6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Schema to your Dataframe\n",
        "\n",
        "StructType, StructField, Spark Datatypes\n",
        "\n",
        "Spark Datatypes are not the same as Python Datatypes\n",
        "\n",
        "string vs StringType<br>\n",
        "int vs IntegerType<br>\n",
        "double vs DoubleType<br>\n",
        "date vs DateType<br>\n",
        "\n",
        "The type classes are in pyspark.sql.types<br>\n",
        "`from pyspark.sql.types import *`\n",
        "\n",
        "#### How to create an empty Dataframe for a given schema ?\n"
      ],
      "metadata": {
        "id": "ci1lwBBrJ56s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "from datetime import datetime\n",
        "schema = StructType(\n",
        "          [StructField(\"name\", StringType(), False),\n",
        "           StructField(\"dob\", DateType(), False)\n",
        "          ]\n",
        "        )\n",
        "\n",
        "df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "2mSh83rj_MUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark dataframe API: Create a dataframe from the csv data source\n",
        "\n",
        "Use pyspark syntax to do data transformation"
      ],
      "metadata": {
        "id": "hVL7FkRMKctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df = spark.read.format(\"csv\").option(\"header\", \"true\").load('./spark_play/netflix_titles.csv')\n",
        "movies_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcOcsqJHKtyR",
        "outputId": "55d3ddd5-0930-46f2-d86a-b3a04ccf8833"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+-----------------+--------------------+-------------+-----------------+------------+------+---------+--------------------+--------------------+\n",
            "|show_id|   type|title|         director|                cast|      country|       date_added|release_year|rating| duration|           listed_in|         description|\n",
            "+-------+-------+-----+-----------------+--------------------+-------------+-----------------+------------+------+---------+--------------------+--------------------+\n",
            "|     s1|TV Show|   3%|             null|João Miguel, Bian...|       Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|International TV ...|In a future where...|\n",
            "|     s2|  Movie| 7:19|Jorge Michel Grau|Demián Bichir, Hé...|       Mexico|December 23, 2016|        2016| TV-MA|   93 min|Dramas, Internati...|After a devastati...|\n",
            "|     s3|  Movie|23:59|     Gilbert Chan|Tedd Chan, Stella...|    Singapore|December 20, 2018|        2011|     R|   78 min|Horror Movies, In...|When an army recr...|\n",
            "|     s4|  Movie|    9|      Shane Acker|Elijah Wood, John...|United States|November 16, 2017|        2009| PG-13|   80 min|Action & Adventur...|In a postapocalyp...|\n",
            "|     s5|  Movie|   21|   Robert Luketic|Jim Sturgess, Kev...|United States|  January 1, 2020|        2008| PG-13|  123 min|              Dramas|A brilliant group...|\n",
            "+-------+-------+-----+-----------------+--------------------+-------------+-----------------+------------+------+---------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataframe Column Operations\n",
        "Add a new Column from existing columns<br>\n",
        "Modify a column<br>\n",
        "Rename a column <br>\n",
        "\n",
        "#### How do you refer a column ?<br>\n",
        "use df.column <br>\n",
        "use col object\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "use when condition in pyspark api for complex conditions\n",
        "\n",
        "#### Try these other functions\n",
        "concat, upper, lower\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EGemGhX0LMOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df.withColumn(\"isDrama\", movies_df.listed_in.contains('Drama'))\n",
        "movies_df.withColumn(\"type\", movies_df.title)\n",
        "movies_df.withColumnRenamed(\"country\", \"released_country\")\n",
        "\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "movies_df.withColumn(\"pre-2000\", col(\"release_year\"))\n",
        "\n",
        "movies_df.withColumn(\"pre-2000\", when(col(\"release_year\")<2010, \"pre-2k\").otherwise(\"post_2000\")  )\n",
        "\n",
        "movies_df.withColumn(\"source\", lit(\"csv\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "magAGym1jM40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5388e4-6d25-4e33-a258-560cecbe673d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[show_id: string, type: string, title: string, director: string, cast: string, country: string, date_added: string, release_year: string, rating: string, duration: string, listed_in: string, description: string, source: string]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Add a prefix to all column names\n"
      ],
      "metadata": {
        "id": "ngHKDSpNLbWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toDF([\"movie_\"+x for x in movies_df.columns])"
      ],
      "metadata": {
        "id": "z-Guj5fywDJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### collect df values as python lists ❌::"
      ],
      "metadata": {
        "id": "yPr4ftQMW1gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp = spark.createDataFrame([[\"ash\", \"2019-01-01\", \"HR\"], [\"mod\", \"2020-01-01\", \"IT\"]], schema=(\"name\", \"dob\", \"dept\"))\n",
        "emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PS-0bC3cHoy",
        "outputId": "c598c6aa-d96f-49f7-cae8-a69d49e1d036"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----+\n",
            "|name|       dob|dept|\n",
            "+----+----------+----+\n",
            "| ash|2019-01-01|  HR|\n",
            "| mod|2020-01-01|  IT|\n",
            "+----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= emp.select('name').collect()\n",
        "print(type(a))\n",
        "print(type(a[0]))\n",
        "print(a[0])\n",
        "print(a[0][0])\n",
        "print(a[1][0])\n",
        "print(a)\n",
        "[_[0] for _ in a]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbcwQK9Aeeux",
        "outputId": "19813b04-236e-410b-8415-3fa8b279c88e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'pyspark.sql.types.Row'>\n",
            "Row(name='ash')\n",
            "ash\n",
            "mod\n",
            "[Row(name='ash'), Row(name='mod')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ash', 'mod']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining UDFs\n",
        "\n",
        "import udf function and return types\n",
        "\n",
        "\n",
        "use lambda function <br>\n",
        "use python function <br>\n",
        "\n",
        "register to use in spark.sql"
      ],
      "metadata": {
        "id": "rfxOjQiEJp6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
        "\n",
        "def get_first_word(x):\n",
        "  return x.split()[0]\n",
        "\n",
        "first_word_udf = udf(get_first_word, StringType())\n",
        "movies_df.withColumn(\"first\", first_word_udf(col(\"listed_in\"))) .select(\"listed_in\" ,'first')\n",
        "\n",
        "movies_df.createOrReplaceGlobalTempView(\"sql_movie\")\n",
        "spark.udf.register(\"super_udf\", get_first_word, StringType())\n",
        "spark.sql(\"select  listed_in, super_udf(listed_in) from sql_movie\").show()"
      ],
      "metadata": {
        "id": "yW21pZ74JKw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JOIN\n",
        "pyspark syntax\n",
        "\n",
        "df1.join(df2, on=[col1], how=\"inner\")\n",
        "\n",
        "```\n",
        "Above works, when both dataframes have the same join columns (name matching).\n",
        "Note the repeating columns in the result.\n",
        "What about the join condition column(s)?\n",
        "```\n",
        "\n",
        "when you want to join on columns with different names.\n",
        "\n",
        "\n",
        "Selcet all columns from one side in simpler way\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YnWqiQ1G2YsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drama_df = movies_df.filter(col(\"listed_in\").contains('Drama'))\n",
        "# inner_df =  movies_df.join(drama_df, on=[\"release_year\"])\n",
        "\n",
        "\n",
        "# drama_df = drama_df.withColumnRenamed(\"release_year\", \"year_produced\")\n",
        "# inner_df =  movies_df.join(drama_df, on=[movies_df.release_year==drama_df.year_produced]).select(drama_df.*).show(5)\n",
        "\n",
        "drama_df = drama_df.withColumnRenamed(\"release_year\", \"year_produced\")\n",
        "movies_df.join(drama_df, on=[movies_df.release_year==drama_df.year_produced]) .select(drama_df[\"*\"], movies_df.release_year ).show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "n0wmnt5t2UUw",
        "outputId": "36b2b621-0e25-4fac-a489-c5879467aae4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'drama_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50285378b26a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# inner_df =  movies_df.join(drama_df, on=[movies_df.release_year==drama_df.year_produced]).select(drama_df.*).show(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrama_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrama_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"release_year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"year_produced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmovies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrama_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease_year\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdrama_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear_produced\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrama_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease_year\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drama_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try other join types.\n",
        "1. left, anti <br>\n",
        "\n",
        "2. Check exclude"
      ],
      "metadata": {
        "id": "6wRioR4YBvFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the below exercises with pyspark API. You may verify your results with SQL API.\n",
        "\n",
        "\n",
        "#### 1. Try filtering by multiple conditions- say 1)year 2010 OR country = India 2) year 2010 and country India\n",
        "\n",
        "#### 2. Get a peek of dataframe by show, first, last\n",
        "\n",
        "#### 3. Select 1 or more columns and drop the duplicates. (there is a simple function)\n",
        "\n",
        "#### 4. Check the groupby syntax. First groupby(col) add aggregations to it as agg(avg(col))\n",
        "\n",
        "#### 5. Can you get the distinct count of countries in each release year . Produce a dataframe showing year, count_countries\n",
        "\n",
        "#### 6. What is the semi join vs anti join types ? Show an example for each. If needed create a simple dataframe using createDataframe method.\n",
        "\n",
        "#### 7. Create a single row dataframe using ['20200714', '2020-07-15', '2020-16-07 23:15']. Convert each of the columns to the date/timestamp values formats applicable. After conversion, printing the target dataframe's schema will be as follows.\n",
        "\n",
        "```\n",
        "source\n",
        " |-- a: string (nullable = true)\n",
        " |-- b: string (nullable = true)\n",
        " |-- c: string (nullable = true)\n",
        "\n",
        "target\n",
        " |-- a: date (nullable = true)\n",
        " |-- b: date (nullable = true)\n",
        " |-- c: timestamp (nullable = true)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N63_RJwJDrrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON in .json\n",
        "\n",
        "dot notation\n",
        "\n",
        "explode"
      ],
      "metadata": {
        "id": "bFjGbqBIm0zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "row = spark.read.format(\"json\").load(\"./spark_play/data/jsonfile.json\")\n",
        "\n",
        "json_image_schema = row.schema\n",
        "\n",
        "row.show(truncate=False)\n",
        "\n",
        "row.select( row[\"Image\"][\"Title\"].alias(\"img_title\"), row[\"Image\"][\"Height\"], row[\"Image\"][\"Width\"]).show()\n",
        "\n",
        "row.select( explode(row[\"Image\"][\"IDs\"]).alias(\"ID\"), row[\"Image\"][\"Height\"] ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6IxJRAOVEft",
        "outputId": "2a76a7c3-cba3-4f42-b52c-38480a3b39b0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------------------+\n",
            "|Image                                                                                                       |\n",
            "+------------------------------------------------------------------------------------------------------------+\n",
            "|{600, [116, 943, 234, 38793], {125, http://www.example.com/image/481989943, 100}, View from 15th Floor, 800}|\n",
            "+------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "+--------------------+------------+-----------+\n",
            "|           img_title|Image.Height|Image.Width|\n",
            "+--------------------+------------+-----------+\n",
            "|View from 15th Floor|         600|        800|\n",
            "+--------------------+------------+-----------+\n",
            "\n",
            "+-----+------------+\n",
            "|   ID|Image.Height|\n",
            "+-----+------------+\n",
            "|  116|         600|\n",
            "|  943|         600|\n",
            "|  234|         600|\n",
            "|38793|         600|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON in txt"
      ],
      "metadata": {
        "id": "fYHltn-gqPru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, from_json\n",
        "\n",
        "arow = (spark.read.format(\"csv\")\n",
        "      .option(\"delimiter\", '|')\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"quote\", '\\'')\n",
        "      .load(\"./spark_play/data/json_in_text.csv\")\n",
        ")\n",
        "\n",
        "arow.printSchema()\n",
        "\n",
        "get_json_schema = spark.read.json(arow.json_image).schema\n",
        "\n",
        "arow.withColumn(\"json_image\",  from_json(arow[\"image\"], schema=get_json_schema )).select(col(\"image_id\"), col(\"json_image.Image.Title\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "1155c39e-cb97-4be2-841e-fd233c40898d",
        "id": "lj5Ta_oKqL9g"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a002a68d9623>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m arow = (spark.read.format(\"csv\")\n\u001b[1;32m      4\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}