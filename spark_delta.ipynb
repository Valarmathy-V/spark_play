{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkYT6K7Bp2T5",
        "outputId": "a93022be-7d35-45ae-b472-2b2c20ad4fed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrIjg86XqYUs",
        "outputId": "c115492a-1ed8-47fd-cdba-9ab788d9c9c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf .git\n",
        "# !git clone https://github.com/bst-depractice/spark_play"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwltLOPWcM48",
        "outputId": "468ced92-33a9-422f-f16d-c6b88d3d4ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spark_play'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 15 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (15/15), 1.17 MiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs"
      },
      "source": [
        "!apt-get -qq update > /tmp/apt.out\n",
        "!apt-get install -y -qq openjdk-11-jdk-headless"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(wget -q --show-progress -nc https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "32LLPBQpij-w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-e1DAsGUaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6add63-2eb7-4386-f91f-9268ed906fdc"
      },
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta, pyngrok\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark==3.2.1\n",
        "  %pip install -q findspark\n",
        "  %pip install -q delta\n",
        "  %pip install pyngrok"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the config k,v pairs and get a spark session object"
      ],
      "metadata": {
        "id": "7CBeyfwbK61a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/drive/MyDrive/Github/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "findspark.init()\n",
        "MAX_MEMORY=\"8g\"\n",
        "maven_coords = [\n",
        "    \"org.apache.spark:spark-avro_2.12:3.2.1\",\n",
        "    \"io.delta:delta-core_2.12:2.0.0rc1\",\n",
        "    \"org.xerial:sqlite-jdbc:3.36.0.3\",\n",
        "    \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\",\n",
        "    \"com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.8\",\n",
        "]\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(maven_coords))\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY)\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
        "    .config('spark.ui.port', '4050')\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        "    )\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nyXkqLzrjJ1c",
        "outputId": "6979c38a-0f10-4ffd-dc2c-94fdd07e4f2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f58683ebc10>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://54d80cebd0c9:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied \"\n",
        "\"from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4040\n",
        "public_url = ngrok.connect(ui_port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{ui_port}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dqyNpYQTDIM",
        "outputId": "02e254f3-6ac5-4613-b5cf-1956318ec998"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-15T14:39:21+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://50f0-34-68-94-133.ngrok-free.app\" -> \"http://127.0.0.1:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup is complete. At this point you have started a spark application and able to access the application-ui using the url above.\n",
        "You can start writing your data transformation code below..."
      ],
      "metadata": {
        "id": "PFIP7ijrWE6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Schema to your Dataframe\n",
        "\n",
        "StructType, StructField, Spark Datatypes\n",
        "\n",
        "Spark Datatypes are not the same as Python Datatypes\n",
        "\n",
        "string vs StringType<br>\n",
        "int vs IntegerType<br>\n",
        "double vs DoubleType<br>\n",
        "date vs DateType<br>\n",
        "\n",
        "The type classes are in pyspark.sql.types<br>\n",
        "`from pyspark.sql.types import *`\n",
        "\n",
        "#### How to create an empty Dataframe for a given schema ?\n"
      ],
      "metadata": {
        "id": "ci1lwBBrJ56s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "from datetime import datetime\n",
        "schema = StructType(\n",
        "          [StructField(\"name\", StringType(), False),\n",
        "           StructField(\"dob\", DateType(), False)\n",
        "          ]\n",
        "        )\n",
        "\n",
        "df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"delta_table\")"
      ],
      "metadata": {
        "id": "2mSh83rj_MUU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge into a delta_table using records from dataframe\n",
        "\n",
        "with pyspark API"
      ],
      "metadata": {
        "id": "hVL7FkRMKctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "\n",
        "schema = StructType(\n",
        "          [StructField(\"name\", StringType(), False),\n",
        "           StructField(\"dob\", DateType(), False)\n",
        "          ]\n",
        "        )\n",
        "df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2010-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "\n",
        "new_df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2026-01-01\", \"%Y-%m-%d\")],\n",
        "                                [\"mat\", datetime.strptime(\"9926-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"my_delta\")\n",
        "new_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"updates_to_delta\")\n",
        "\n",
        "spark.sql(\"select * from my_delta\").show()\n",
        "\n",
        "spark.sql(\"\"\"Merge into my_delta\n",
        "              using updates_to_delta\n",
        "              on my_delta.name = updates_to_delta.name\n",
        "              when matched then\n",
        "                update set\n",
        "                  dob = updates_to_delta.dob\n",
        "              when not matched then\n",
        "                insert (name, dob)  values (updates_to_delta.name, updates_to_delta.dob)\n",
        "\n",
        "            \"\"\")\n",
        "\n",
        "spark.sql(\"select * from my_delta\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcOcsqJHKtyR",
        "outputId": "a61b2768-8a98-4a0a-a472-d041a6eb224b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|name|       dob|\n",
            "+----+----------+\n",
            "| ash|2010-01-01|\n",
            "+----+----------+\n",
            "\n",
            "+----+----------+\n",
            "|name|       dob|\n",
            "+----+----------+\n",
            "| ash|2026-01-01|\n",
            "| mat|9926-01-01|\n",
            "+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge into a delta_table using records from dataframe\n",
        "\n",
        "with SQL API"
      ],
      "metadata": {
        "id": "Vothy4PxCIaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "\n",
        "schema = StructType(\n",
        "          [StructField(\"name\", StringType(), False),\n",
        "           StructField(\"dob\", DateType(), False)\n",
        "          ]\n",
        "        )\n",
        "df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2010-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "\n",
        "new_df = spark.createDataFrame([[\"ash\", datetime.strptime(\"2026-01-01\", \"%Y-%m-%d\")],\n",
        "                                [\"mat\", datetime.strptime(\"9926-01-01\", \"%Y-%m-%d\")]], schema = schema)\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"my_delta\")\n",
        "new_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"updates_to_delta\")\n",
        "\n",
        "spark.sql(\"select * from my_delta\").show()\n",
        "\n",
        "spark.sql(\"\"\"Merge into my_delta\n",
        "              using updates_to_delta\n",
        "              on my_delta.name = updates_to_delta.name\n",
        "              when matched then\n",
        "                update set\n",
        "                  dob = updates_to_delta.dob\n",
        "              when not matched then\n",
        "                insert (name, dob)  values (updates_to_delta.name, updates_to_delta.dob)\n",
        "\n",
        "            \"\"\")\n",
        "\n",
        "spark.sql(\"select * from my_delta\").show()"
      ],
      "metadata": {
        "id": "ngN8bPfSH2_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}